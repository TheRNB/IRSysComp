{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to design an IR system on the given dataset which can be found [here](http://ai.stanford.edu/~amaas/data/sentiment/ \"link to dataset\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: dataset input\n",
    "reading the files from the dataset. In order to do this step we use the OS library and list all the files available in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715_9.txt \n",
      " 1821_4.txt\n",
      "4715_9.txt 1821_4.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Path to directory\n",
    "pos_directory_path = \"./docs/aclImdb/train/pos\"\n",
    "neg_directory_path = \"./docs/aclImdb/train/neg\"\n",
    "pos_test_path = \"./docs/aclImdb/test/pos\"\n",
    "neg_test_path = \"./docs/aclImdb/test/neg\"\n",
    "\n",
    "#Getting a list of files in the directory\n",
    "pos_file_names = os.listdir(pos_directory_path)\n",
    "neg_file_names = os.listdir(neg_directory_path)\n",
    "pos_test_file_names = os.listdir(pos_test_path)\n",
    "neg_test_file_names = os.listdir(neg_test_path)\n",
    "\n",
    "#DEBUG: checking the list\n",
    "print(pos_file_names[0],\"\\n\", neg_file_names[0])\n",
    "\n",
    "print(pos_test_file_names[0], neg_test_file_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the names, we can start by reading the content of every file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
      "[\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\"]\n",
      "['Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" <br /><br />Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. <br /><br />Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. <br /><br />\"We are taught that suffering is the one promise that life always keeps.\"']\n",
      "[\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It's worth seeing for their scenes- and Rickman's scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\"]\n"
     ]
    }
   ],
   "source": [
    "#list of document content\n",
    "pos_file_content = []\n",
    "\n",
    "for file_name in pos_file_names:\n",
    "    with open(pos_directory_path+\"/\"+file_name, 'r', encoding='charmap') as file:\n",
    "        pos_file_content.append(file.readlines())\n",
    "        #print(file_content[-1])\n",
    "\n",
    "neg_file_content = []\n",
    "\n",
    "for file_name in neg_file_names:\n",
    "    with open(neg_directory_path+\"/\"+file_name, 'r', encoding='charmap') as file:\n",
    "        neg_file_content.append(file.readlines())\n",
    "        #print(file_content[-1])\n",
    "\n",
    "pos_test_file_content = []\n",
    "\n",
    "for file_name in pos_test_file_names:\n",
    "    with open(pos_test_path+\"/\"+file_name, 'r', encoding='charmap') as file:\n",
    "        pos_test_file_content.append(file.readlines())\n",
    "        #print(file_content[-1])\n",
    "\n",
    "neg_test_file_content = []\n",
    "\n",
    "for file_name in neg_test_file_names:\n",
    "    with open(neg_test_path+\"/\"+file_name, 'r', encoding='charmap') as file:\n",
    "        neg_test_file_content.append(file.readlines())\n",
    "        #print(file_content[-1])\n",
    "    \n",
    "print(pos_file_content[0])\n",
    "print(neg_file_content[0])\n",
    "print(pos_test_file_content[0])\n",
    "print(neg_test_file_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: tokenizing\n",
    "Now that we have the text in our program we should start by tokenizing the text.\n",
    "In order to do this we're going to use the nltk library in python and to make our job easier we're going to tokenize words separated by Space, Comma, and dash.\n",
    "\n",
    "Examples:\n",
    "1. I live ...\n",
    "2. Student, jason, ...\n",
    "3. 30-year-old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream', 'Watch', 'for', 'Alan', 'The', 'Skipper', 'Hale', 'jr', 'as', 'a', 'police', 'Sgt']\n",
      "['Working', 'with', 'one', 'of', 'the', 'best', 'Shakespeare', 'sources', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', 'it', 's', 'source', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', 'br', 'br', 'Branagh', 'steals', 'the', 'film', 'from', 'under', 'Fishburne', 's', 'nose', 'and', 'there', 's', 'a', 'talented', 'cast', 'on', 'good', 'form']\n",
      "['Based', 'on', 'an', 'actual', 'story', 'John', 'Boorman', 'shows', 'the', 'struggle', 'of', 'an', 'American', 'doctor', 'whose', 'husband', 'and', 'son', 'were', 'murdered', 'and', 'she', 'was', 'continually', 'plagued', 'with', 'her', 'loss', 'A', 'holiday', 'to', 'Burma', 'with', 'her', 'sister', 'seemed', 'like', 'a', 'good', 'idea', 'to', 'get', 'away', 'from', 'it', 'all', 'but', 'when', 'her', 'passport', 'was', 'stolen', 'in', 'Rangoon', 'she', 'could', 'not', 'leave', 'the', 'country', 'with', 'her', 'sister', 'and', 'was', 'forced', 'to', 'stay', 'back', 'until', 'she', 'could', 'get', 'I', 'D', 'papers', 'from', 'the', 'American', 'embassy', 'To', 'fill', 'in', 'a', 'day', 'before', 'she', 'could', 'fly', 'out', 'she', 'took', 'a', 'trip', 'into', 'the', 'countryside', 'with', 'a', 'tour', 'guide', 'I', 'tried', 'finding', 'something', 'in', 'those', 'stone', 'statues', 'but', 'nothing', 'stirred', 'in', 'me', 'I', 'was', 'stone', 'myself', 'br', 'br', 'Suddenly', 'all', 'hell', 'broke', 'loose', 'and', 'she', 'was', 'caught', 'in', 'a', 'political', 'revolt', 'Just', 'when', 'it', 'looked', 'like', 'she', 'had', 'escaped', 'and', 'safely', 'boarded', 'a', 'train', 'she', 'saw', 'her', 'tour', 'guide', 'get', 'beaten', 'and', 'shot', 'In', 'a', 'split', 'second', 'she', 'decided', 'to', 'jump', 'from', 'the', 'moving', 'train', 'and', 'try', 'to', 'rescue', 'him', 'with', 'no', 'thought', 'of', 'herself', 'Continually', 'her', 'life', 'was', 'in', 'danger', 'br', 'br', 'Here', 'is', 'a', 'woman', 'who', 'demonstrated', 'spontaneous', 'selfless', 'charity', 'risking', 'her', 'life', 'to', 'save', 'another', 'Patricia', 'Arquette', 'is', 'beautiful', 'and', 'not', 'just', 'to', 'look', 'at', 'she', 'has', 'a', 'beautiful', 'heart', 'This', 'is', 'an', 'unforgettable', 'story', 'br', 'br', 'We', 'are', 'taught', 'that', 'suffering', 'is', 'the', 'one', 'promise', 'that', 'life', 'always', 'keeps']\n",
      "['Alan', 'Rickman', 'Emma', 'Thompson', 'give', 'good', 'performances', 'with', 'southern', 'New', 'Orleans', 'accents', 'in', 'this', 'detective', 'flick', 'It', 's', 'worth', 'seeing', 'for', 'their', 'scenes', 'and', 'Rickman', 's', 'scene', 'with', 'Hal', 'Holbrook', 'These', 'three', 'actors', 'mannage', 'to', 'entertain', 'us', 'no', 'matter', 'what', 'the', 'movie', 'it', 'seems', 'The', 'plot', 'for', 'the', 'movie', 'shows', 'potential', 'but', 'one', 'gets', 'the', 'impression', 'in', 'watching', 'the', 'film', 'that', 'it', 'was', 'not', 'pulled', 'off', 'as', 'well', 'as', 'it', 'could', 'have', 'been', 'The', 'fact', 'that', 'it', 'is', 'cluttered', 'by', 'a', 'rather', 'uninteresting', 'subplot', 'and', 'mostly', 'uninteresting', 'kidnappers', 'really', 'muddles', 'things', 'The', 'movie', 'is', 'worth', 'a', 'view', 'if', 'for', 'nothing', 'more', 'than', 'entertaining', 'performances', 'by', 'Rickman', 'Thompson', 'and', 'Holbrook']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# A list for tokenized texts\n",
    "pos_file_content_tokenized = []\n",
    "\n",
    "# Define a regular expression pattern to match words separated by Space, Comma, and Dash\n",
    "#pattern = r'\\w+|\\$[\\d\\.]+'\n",
    "pattern = r'\\d{1,4}(?:,\\d{3})*(?:\\.\\d+)?|\\w+'\n",
    "#pattern = r'[A-Za-z0-9]+.*[A-Za-z0-9]+.*([+-]?(?=\\.\\d|\\d)(?:\\d+)?(?:\\.?\\d*))(?:[Ee]([+-]?\\d+))?.*([0-9]+(,[0-9]+)+)'\n",
    "\n",
    "#Change all input data into tokens\n",
    "for file in pos_file_content:\n",
    "    # Use regex_tokenize with the defined pattern\n",
    "    pos_file_content_tokenized.append(RegexpTokenizer(pattern).tokenize(file[0]))\n",
    "    #print(file_content_tokenized[-1])\n",
    "\n",
    "# A list for tokenized texts\n",
    "neg_file_content_tokenized = []\n",
    "\n",
    "for file in neg_file_content:\n",
    "    # Use regex_tokenize with the defined pattern\n",
    "    neg_file_content_tokenized.append(RegexpTokenizer(pattern).tokenize(file[0]))\n",
    "    #print(file_content_tokenized[-1])\n",
    "\n",
    "# A list for tokenized texts\n",
    "pos_test_file_content_tokenized = []\n",
    "\n",
    "for file in pos_test_file_content:\n",
    "    # Use regex_tokenize with the defined pattern\n",
    "    pos_test_file_content_tokenized.append(RegexpTokenizer(pattern).tokenize(file[0]))\n",
    "    #print(file_content_tokenized[-1])\n",
    "\n",
    "# A list for tokenized texts\n",
    "neg_test_file_content_tokenized = []\n",
    "\n",
    "for file in neg_test_file_content:\n",
    "    # Use regex_tokenize with the defined pattern\n",
    "    neg_test_file_content_tokenized.append(RegexpTokenizer(pattern).tokenize(file[0]))\n",
    "    #print(file_content_tokenized[-1])\n",
    "\n",
    "print(pos_file_content_tokenized[0])\n",
    "print(neg_file_content_tokenized[0])\n",
    "print(pos_test_file_content_tokenized[0])\n",
    "print(neg_test_file_content_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: lowercasing\n",
    "We now want to lowercase our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', 'as', 'a', 'police', 'sgt']\n",
      "['working', 'with', 'one', 'of', 'the', 'best', 'shakespeare', 'sources', 'this', 'film', 'manages', 'to', 'be', 'creditable', 'to', 'it', 's', 'source', 'whilst', 'still', 'appealing', 'to', 'a', 'wider', 'audience', 'br', 'br', 'branagh', 'steals', 'the', 'film', 'from', 'under', 'fishburne', 's', 'nose', 'and', 'there', 's', 'a', 'talented', 'cast', 'on', 'good', 'form']\n",
      "['based', 'on', 'an', 'actual', 'story', 'john', 'boorman', 'shows', 'the', 'struggle', 'of', 'an', 'american', 'doctor', 'whose', 'husband', 'and', 'son', 'were', 'murdered', 'and', 'she', 'was', 'continually', 'plagued', 'with', 'her', 'loss', 'a', 'holiday', 'to', 'burma', 'with', 'her', 'sister', 'seemed', 'like', 'a', 'good', 'idea', 'to', 'get', 'away', 'from', 'it', 'all', 'but', 'when', 'her', 'passport', 'was', 'stolen', 'in', 'rangoon', 'she', 'could', 'not', 'leave', 'the', 'country', 'with', 'her', 'sister', 'and', 'was', 'forced', 'to', 'stay', 'back', 'until', 'she', 'could', 'get', 'i', 'd', 'papers', 'from', 'the', 'american', 'embassy', 'to', 'fill', 'in', 'a', 'day', 'before', 'she', 'could', 'fly', 'out', 'she', 'took', 'a', 'trip', 'into', 'the', 'countryside', 'with', 'a', 'tour', 'guide', 'i', 'tried', 'finding', 'something', 'in', 'those', 'stone', 'statues', 'but', 'nothing', 'stirred', 'in', 'me', 'i', 'was', 'stone', 'myself', 'br', 'br', 'suddenly', 'all', 'hell', 'broke', 'loose', 'and', 'she', 'was', 'caught', 'in', 'a', 'political', 'revolt', 'just', 'when', 'it', 'looked', 'like', 'she', 'had', 'escaped', 'and', 'safely', 'boarded', 'a', 'train', 'she', 'saw', 'her', 'tour', 'guide', 'get', 'beaten', 'and', 'shot', 'in', 'a', 'split', 'second', 'she', 'decided', 'to', 'jump', 'from', 'the', 'moving', 'train', 'and', 'try', 'to', 'rescue', 'him', 'with', 'no', 'thought', 'of', 'herself', 'continually', 'her', 'life', 'was', 'in', 'danger', 'br', 'br', 'here', 'is', 'a', 'woman', 'who', 'demonstrated', 'spontaneous', 'selfless', 'charity', 'risking', 'her', 'life', 'to', 'save', 'another', 'patricia', 'arquette', 'is', 'beautiful', 'and', 'not', 'just', 'to', 'look', 'at', 'she', 'has', 'a', 'beautiful', 'heart', 'this', 'is', 'an', 'unforgettable', 'story', 'br', 'br', 'we', 'are', 'taught', 'that', 'suffering', 'is', 'the', 'one', 'promise', 'that', 'life', 'always', 'keeps']\n",
      "['alan', 'rickman', 'emma', 'thompson', 'give', 'good', 'performances', 'with', 'southern', 'new', 'orleans', 'accents', 'in', 'this', 'detective', 'flick', 'it', 's', 'worth', 'seeing', 'for', 'their', 'scenes', 'and', 'rickman', 's', 'scene', 'with', 'hal', 'holbrook', 'these', 'three', 'actors', 'mannage', 'to', 'entertain', 'us', 'no', 'matter', 'what', 'the', 'movie', 'it', 'seems', 'the', 'plot', 'for', 'the', 'movie', 'shows', 'potential', 'but', 'one', 'gets', 'the', 'impression', 'in', 'watching', 'the', 'film', 'that', 'it', 'was', 'not', 'pulled', 'off', 'as', 'well', 'as', 'it', 'could', 'have', 'been', 'the', 'fact', 'that', 'it', 'is', 'cluttered', 'by', 'a', 'rather', 'uninteresting', 'subplot', 'and', 'mostly', 'uninteresting', 'kidnappers', 'really', 'muddles', 'things', 'the', 'movie', 'is', 'worth', 'a', 'view', 'if', 'for', 'nothing', 'more', 'than', 'entertaining', 'performances', 'by', 'rickman', 'thompson', 'and', 'holbrook']\n"
     ]
    }
   ],
   "source": [
    "#lowercasing each token individually\n",
    "for i in range(len(pos_file_content_tokenized)):\n",
    "    for j in range(len(pos_file_content_tokenized[i])):\n",
    "        pos_file_content_tokenized[i][j] = pos_file_content_tokenized[i][j].lower()\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "#lowercasing each token individually\n",
    "for i in range(len(neg_file_content_tokenized)):\n",
    "    for j in range(len(neg_file_content_tokenized[i])):\n",
    "        neg_file_content_tokenized[i][j] = neg_file_content_tokenized[i][j].lower()\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "        \n",
    "#lowercasing each token individually\n",
    "for i in range(len(pos_test_file_content_tokenized)):\n",
    "    for j in range(len(pos_test_file_content_tokenized[i])):\n",
    "        pos_test_file_content_tokenized[i][j] = pos_test_file_content_tokenized[i][j].lower()\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "#lowercasing each token individually\n",
    "for i in range(len(neg_test_file_content_tokenized)):\n",
    "    for j in range(len(neg_test_file_content_tokenized[i])):\n",
    "        neg_test_file_content_tokenized[i][j] = neg_test_file_content_tokenized[i][j].lower()\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "\n",
    "print(pos_file_content_tokenized[0])\n",
    "print(neg_file_content_tokenized[0])\n",
    "print(pos_test_file_content_tokenized[0])\n",
    "print(neg_test_file_content_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: stopword removal\n",
    "Now we want to remove the stopwords present in our text. To do this task we use the library of stopwords in nltk library. Please note that in order to be able to perform positional queries, we will not delete stopwords, just replace them with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you'll\", 'about', 'its', 'ours', \"aren't\", \"she's\", 'weren', 'were', 'and', 'whom', 'won', 'a', 'into', 'some', \"won't\", 'he', 'further', \"shan't\", 'haven', 'very', 'which', 'to', 'theirs', 'mustn', 'few', 'hers', 'other', 'being', 'hadn', 've', 'any', 'just', \"haven't\", 'aren', 'this', 'of', 'their', 'against', 'ourselves', \"wouldn't\", \"wasn't\", 'those', \"shouldn't\", 'himself', 'my', \"hadn't\", \"hasn't\", 'm', 'between', 'only', 'needn', 'but', 'once', 'll', 'while', 'shan', 'who', 'why', 'your', 'before', \"couldn't\", 'such', 'by', 'so', 'here', 'you', 'y', 'more', 'don', 'wasn', 't', 'too', \"isn't\", 'have', 'am', 'than', 'them', 'own', 'be', 'shouldn', 'me', 'under', 'our', 'wouldn', 'from', 'was', \"should've\", \"needn't\", 'through', 'hasn', 'until', 'during', \"weren't\", 'when', \"you've\", 'off', 'myself', \"you're\", 'it', 'd', 'in', 'isn', 'had', 'how', \"mightn't\", 'themselves', 'been', 'on', 'mightn', \"you'd\", \"mustn't\", 'where', 'all', 'yourselves', 'above', 'they', 'not', 'will', 'should', 'same', 'i', 'that', 'if', 'these', 'again', 'as', 'itself', 'having', \"that'll\", \"didn't\", 'below', 'after', 'does', 'down', 'she', 'up', 'doesn', 'most', 'for', 'doing', 'o', 'him', 'has', 'ma', 'are', 'can', 'an', 'nor', 'yourself', 'the', 'each', 'yours', 're', 'couldn', 'because', 'didn', 'his', 'with', 'over', 'is', 'out', 'no', 'at', 's', 'we', 'did', 'both', 'now', 'what', \"don't\", 'do', 'herself', 'her', 'or', 'then', \"it's\", 'there', 'ain', \"doesn't\"}\n",
      "['movie', 'gets', 'respect', 'sure', 'lot', 'memorable', 'quotes', 'listed', 'gem', 'imagine', 'movie', 'joe', 'piscopo', 'actually', 'funny', 'maureen', 'stapleton', 'scene', 'stealer', 'moroni', 'character', 'absolute', 'scream', 'watch', 'alan', 'skipper', 'hale', 'jr', 'police', 'sgt']\n",
      "['working', 'one', 'best', 'shakespeare', 'sources', 'film', 'manages', 'creditable', 'source', 'whilst', 'still', 'appealing', 'wider', 'audience', 'br', 'br', 'branagh', 'steals', 'film', 'fishburne', 'nose', 'talented', 'cast', 'good', 'form']\n",
      "['based', 'actual', 'story', 'john', 'boorman', 'shows', 'struggle', 'american', 'doctor', 'whose', 'husband', 'son', 'murdered', 'continually', 'plagued', 'loss', 'holiday', 'burma', 'sister', 'seemed', 'like', 'good', 'idea', 'get', 'away', 'passport', 'stolen', 'rangoon', 'could', 'leave', 'country', 'sister', 'forced', 'stay', 'back', 'could', 'get', 'papers', 'american', 'embassy', 'fill', 'day', 'could', 'fly', 'took', 'trip', 'countryside', 'tour', 'guide', 'tried', 'finding', 'something', 'stone', 'statues', 'nothing', 'stirred', 'stone', 'br', 'br', 'suddenly', 'hell', 'broke', 'loose', 'caught', 'political', 'revolt', 'looked', 'like', 'escaped', 'safely', 'boarded', 'train', 'saw', 'tour', 'guide', 'get', 'beaten', 'shot', 'split', 'second', 'decided', 'jump', 'moving', 'train', 'try', 'rescue', 'thought', 'continually', 'life', 'danger', 'br', 'br', 'woman', 'demonstrated', 'spontaneous', 'selfless', 'charity', 'risking', 'life', 'save', 'another', 'patricia', 'arquette', 'beautiful', 'look', 'beautiful', 'heart', 'unforgettable', 'story', 'br', 'br', 'taught', 'suffering', 'one', 'promise', 'life', 'always', 'keeps']\n",
      "['alan', 'rickman', 'emma', 'thompson', 'give', 'good', 'performances', 'southern', 'new', 'orleans', 'accents', 'detective', 'flick', 'worth', 'seeing', 'scenes', 'rickman', 'scene', 'hal', 'holbrook', 'three', 'actors', 'mannage', 'entertain', 'us', 'matter', 'movie', 'seems', 'plot', 'movie', 'shows', 'potential', 'one', 'gets', 'impression', 'watching', 'film', 'pulled', 'well', 'could', 'fact', 'cluttered', 'rather', 'uninteresting', 'subplot', 'mostly', 'uninteresting', 'kidnappers', 'really', 'muddles', 'things', 'movie', 'worth', 'view', 'nothing', 'entertaining', 'performances', 'rickman', 'thompson', 'holbrook']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Setting the stopword language\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "\n",
    "for i in range(len(pos_file_content_tokenized)):\n",
    "    j = 0\n",
    "    #print(file_content_tokenized[i])\n",
    "    while j < len(pos_file_content_tokenized[i]):\n",
    "        if pos_file_content_tokenized[i][j] in stop_words:\n",
    "            #print(\"got here with \", file_content_tokenized[i][j])\n",
    "            pos_file_content_tokenized[i] = pos_file_content_tokenized[i][:j]+pos_file_content_tokenized[i][j+1:]\n",
    "            #pos_file_content_tokenized[i][j] = \"\"\n",
    "        else:\n",
    "            j += 1\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "for i in range(len(neg_file_content_tokenized)):\n",
    "    j = 0\n",
    "    #print(file_content_tokenized[i])\n",
    "    while j < len(neg_file_content_tokenized[i]):\n",
    "        if neg_file_content_tokenized[i][j] in stop_words:\n",
    "            #print(\"got here with \", file_content_tokenized[i][j])\n",
    "            neg_file_content_tokenized[i] = neg_file_content_tokenized[i][:j]+neg_file_content_tokenized[i][j+1:]\n",
    "            #neg_file_content_tokenized[i][j] = \"\"\n",
    "        else:\n",
    "            j += 1\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "            \n",
    "for i in range(len(pos_test_file_content_tokenized)):\n",
    "    j = 0\n",
    "    #print(file_content_tokenized[i])\n",
    "    while j < len(pos_test_file_content_tokenized[i]):\n",
    "        if pos_test_file_content_tokenized[i][j] in stop_words:\n",
    "            #print(\"got here with \", file_content_tokenized[i][j])\n",
    "            pos_test_file_content_tokenized[i] = pos_test_file_content_tokenized[i][:j]+pos_test_file_content_tokenized[i][j+1:]\n",
    "            #pos_file_content_tokenized[i][j] = \"\"\n",
    "        else:\n",
    "            j += 1\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "for i in range(len(neg_test_file_content_tokenized)):\n",
    "    j = 0\n",
    "    #print(file_content_tokenized[i])\n",
    "    while j < len(neg_test_file_content_tokenized[i]):\n",
    "        if neg_test_file_content_tokenized[i][j] in stop_words:\n",
    "            #print(\"got here with \", file_content_tokenized[i][j])\n",
    "            neg_test_file_content_tokenized[i] = neg_test_file_content_tokenized[i][:j]+neg_test_file_content_tokenized[i][j+1:]\n",
    "            #neg_file_content_tokenized[i][j] = \"\"\n",
    "        else:\n",
    "            j += 1\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "\n",
    "print(pos_file_content_tokenized[0])\n",
    "print(neg_file_content_tokenized[0])\n",
    "print(pos_test_file_content_tokenized[0])\n",
    "print(neg_test_file_content_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: stemming\n",
    "Now that we have tokenized, lowercased, and removed stopwords, we are going to stem the words to optimize our IR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'get', 'respect', 'sure', 'lot', 'memor', 'quot', 'list', 'gem', 'imagin', 'movi', 'joe', 'piscopo', 'actual', 'funni', 'maureen', 'stapleton', 'scene', 'stealer', 'moroni', 'charact', 'absolut', 'scream', 'watch', 'alan', 'skipper', 'hale', 'jr', 'polic', 'sgt']\n",
      "['work', 'one', 'best', 'shakespear', 'sourc', 'film', 'manag', 'credit', 'sourc', 'whilst', 'still', 'appeal', 'wider', 'audienc', 'br', 'br', 'branagh', 'steal', 'film', 'fishburn', 'nose', 'talent', 'cast', 'good', 'form']\n",
      "['base', 'actual', 'stori', 'john', 'boorman', 'show', 'struggl', 'american', 'doctor', 'whose', 'husband', 'son', 'murder', 'continu', 'plagu', 'loss', 'holiday', 'burma', 'sister', 'seem', 'like', 'good', 'idea', 'get', 'away', 'passport', 'stolen', 'rangoon', 'could', 'leav', 'countri', 'sister', 'forc', 'stay', 'back', 'could', 'get', 'paper', 'american', 'embassi', 'fill', 'day', 'could', 'fli', 'took', 'trip', 'countrysid', 'tour', 'guid', 'tri', 'find', 'someth', 'stone', 'statu', 'noth', 'stir', 'stone', 'br', 'br', 'suddenli', 'hell', 'broke', 'loos', 'caught', 'polit', 'revolt', 'look', 'like', 'escap', 'safe', 'board', 'train', 'saw', 'tour', 'guid', 'get', 'beaten', 'shot', 'split', 'second', 'decid', 'jump', 'move', 'train', 'tri', 'rescu', 'thought', 'continu', 'life', 'danger', 'br', 'br', 'woman', 'demonstr', 'spontan', 'selfless', 'chariti', 'risk', 'life', 'save', 'anoth', 'patricia', 'arquett', 'beauti', 'look', 'beauti', 'heart', 'unforgett', 'stori', 'br', 'br', 'taught', 'suffer', 'one', 'promis', 'life', 'alway', 'keep']\n",
      "['alan', 'rickman', 'emma', 'thompson', 'give', 'good', 'perform', 'southern', 'new', 'orlean', 'accent', 'detect', 'flick', 'worth', 'see', 'scene', 'rickman', 'scene', 'hal', 'holbrook', 'three', 'actor', 'mannag', 'entertain', 'us', 'matter', 'movi', 'seem', 'plot', 'movi', 'show', 'potenti', 'one', 'get', 'impress', 'watch', 'film', 'pull', 'well', 'could', 'fact', 'clutter', 'rather', 'uninterest', 'subplot', 'mostli', 'uninterest', 'kidnapp', 'realli', 'muddl', 'thing', 'movi', 'worth', 'view', 'noth', 'entertain', 'perform', 'rickman', 'thompson', 'holbrook']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initializing the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Stemming each token individually\n",
    "for i in range(len(pos_file_content_tokenized)):\n",
    "    for j in range(len(pos_file_content_tokenized[i])):\n",
    "        pos_file_content_tokenized[i][j] = stemmer.stem(pos_file_content_tokenized[i][j])\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "#Stemming each token individually\n",
    "for i in range(len(neg_file_content_tokenized)):\n",
    "    for j in range(len(neg_file_content_tokenized[i])):\n",
    "        neg_file_content_tokenized[i][j] = stemmer.stem(neg_file_content_tokenized[i][j])\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "\n",
    "#Stemming each token individually\n",
    "for i in range(len(pos_test_file_content_tokenized)):\n",
    "    for j in range(len(pos_test_file_content_tokenized[i])):\n",
    "        pos_test_file_content_tokenized[i][j] = stemmer.stem(pos_test_file_content_tokenized[i][j])\n",
    "    #print(pos_file_content_tokenized[i])\n",
    "\n",
    "#Stemming each token individually\n",
    "for i in range(len(neg_test_file_content_tokenized)):\n",
    "    for j in range(len(neg_test_file_content_tokenized[i])):\n",
    "        neg_test_file_content_tokenized[i][j] = stemmer.stem(neg_test_file_content_tokenized[i][j])\n",
    "    #print(neg_file_content_tokenized[i])\n",
    "        \n",
    "print(pos_file_content_tokenized[0])\n",
    "print(neg_file_content_tokenized[0])\n",
    "print(pos_test_file_content_tokenized[0])\n",
    "print(neg_test_file_content_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: ['i', 'love', 'movi'], Sentiment: 1\n",
      "Comment: ['movi', 'terrible'], Sentiment: -1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.positive_word_counts = {}\n",
    "        self.negative_word_counts = {}\n",
    "        self.positive_total_words = 0\n",
    "        self.negative_total_words = 0\n",
    "        self.positive_doc_count = 12500\n",
    "        self.negative_doc_count = 12500\n",
    "        self.unique_words = set()\n",
    "\n",
    "    def train(self, training_data, label):\n",
    "        for doc in training_data:\n",
    "            self.unique_words.update(set(doc))\n",
    "            if label == 'positive':\n",
    "                for word in doc:\n",
    "                    self.positive_word_counts[word] = self.positive_word_counts.get(word, 0) + 1\n",
    "                    self.positive_total_words += 1\n",
    "            elif label == 'negative':\n",
    "                for word in doc:\n",
    "                    self.negative_word_counts[word] = self.negative_word_counts.get(word, 0) + 1\n",
    "                    self.negative_total_words += 1\n",
    "\n",
    "    def predict(self, document):\n",
    "        positive_score = self.calculate_score(document, self.positive_word_counts, self.positive_total_words, self.positive_doc_count)\n",
    "        negative_score = self.calculate_score(document, self.negative_word_counts, self.negative_total_words, self.negative_doc_count)\n",
    "        return +1 if positive_score > negative_score else -1\n",
    "\n",
    "    def calculate_score(self, words, class_word_counts, total_class_words, total_documents):\n",
    "        score = 0\n",
    "        for word in words:\n",
    "            if word in class_word_counts:\n",
    "                score += math.log((class_word_counts[word] + 1) / (total_class_words + len(self.unique_words)))\n",
    "            else:\n",
    "                score += math.log(1 / (total_class_words + len(self.unique_words)))\n",
    "        return score + math.log(total_documents / (total_documents + 1))\n",
    "\n",
    "\n",
    "test_data = [\n",
    "    [\"i\", \"love\", \"movi\"],\n",
    "    [\"movi\", \"terrible\"]\n",
    "]\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(pos_file_content_tokenized, 'positive')\n",
    "classifier.train(neg_file_content_tokenized, 'negative')\n",
    "\n",
    "for comment in test_data:\n",
    "    prediction = classifier.predict(comment)\n",
    "    print(f\"Comment: {comment}, Sentiment: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: SVM\n",
    "I went and read about these models on the internet, and examined the dataset. In the dataset's readme it says \"Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels.\". As a result, I was thinking about a model that would perform well on out-of-vocabulary words that might be seen in the new reviews. So I chose FastText as my embedding.\n",
    "\n",
    "### FastTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for 'movi': [ 5.94382137e-02 -6.93815053e-02  8.76626298e-02 -2.58051027e-02\n",
      " -1.04652308e-02  3.32218246e-03  3.49742025e-02  8.78931582e-03\n",
      "  1.62410717e-02  9.05352645e-03 -8.31956640e-02 -8.50495175e-02\n",
      "  9.76153836e-02  5.32090887e-02 -7.14431098e-03 -7.19319723e-05\n",
      " -4.90409695e-02 -3.78579982e-02  2.10706033e-02  5.11915125e-02\n",
      " -3.95214558e-03  1.28477633e-01  1.09286286e-01  1.92991421e-02\n",
      "  1.69072114e-02 -2.83123692e-03 -3.99183184e-02  3.13998722e-02\n",
      " -3.89131904e-02 -1.53577467e-02  1.20135613e-01  1.62466578e-02\n",
      "  7.05937073e-02  9.24285196e-05 -1.02586821e-01 -4.31219451e-02\n",
      "  3.19003277e-02  1.22528762e-01 -6.38419613e-02  6.52723685e-02\n",
      "  1.09242417e-01  7.60026127e-02  1.26699299e-01  1.04860738e-01\n",
      "  5.52941998e-03  2.34023947e-02 -2.83986572e-02 -1.72175735e-03\n",
      "  6.78249374e-02 -3.44418623e-02  6.55278191e-02  6.56991303e-02\n",
      " -2.36476474e-02 -1.08757047e-02  1.18466588e-02  1.04738504e-03\n",
      " -3.13774198e-02 -5.65462746e-02  7.79256076e-02 -6.54736236e-02\n",
      " -1.07790090e-01  8.15538168e-02  6.77045584e-02 -6.70841262e-02\n",
      "  5.86164445e-02 -5.38202301e-02  3.62486299e-03 -3.24171335e-02\n",
      " -9.09082517e-02 -1.71441387e-03  5.36351837e-02 -5.12742735e-02\n",
      "  1.46578047e-02  1.15374826e-01 -8.98661092e-02 -1.23635285e-01\n",
      "  6.35366663e-02  3.62296738e-02  1.07890004e-02  8.38551223e-02\n",
      "  3.20192128e-02  2.66867932e-02  2.81111654e-02 -2.48160819e-03\n",
      " -7.60883316e-02 -2.21677907e-02 -7.97662809e-02 -7.60225356e-02\n",
      "  1.10077588e-02 -2.86984444e-02  9.83437300e-02 -1.82025731e-02\n",
      " -5.98702542e-02 -1.48327136e-02  1.10960007e-03  3.47714648e-02\n",
      "  4.84439842e-02  1.41114974e-02  1.78295560e-02 -2.69191638e-02\n",
      "  1.15530600e-03  1.59652438e-02 -2.49425489e-02 -2.21778043e-02\n",
      "  9.83857587e-02 -9.42648575e-02 -4.13946584e-02 -8.27358663e-02\n",
      "  6.26774207e-02  4.73863743e-02 -2.01154072e-02 -7.46517405e-02\n",
      " -1.50515169e-01 -2.83447728e-02  2.93083321e-02 -5.39455861e-02\n",
      " -4.84727956e-02 -1.67608529e-03 -5.51372655e-02 -1.96403731e-03\n",
      " -1.08172178e-01  6.03217147e-02 -6.47705495e-02 -1.20843388e-01\n",
      "  1.23221606e-01  1.95454776e-01  8.00327808e-02  1.32445134e-02\n",
      "  1.08633608e-01  5.47917373e-02 -3.19552161e-02 -3.59670408e-02\n",
      "  1.06173277e-01 -6.19937629e-02  9.01555866e-02  1.50123015e-01\n",
      "  3.48227285e-02  1.10544648e-03  3.18926969e-03 -4.41176863e-03\n",
      "  1.86880585e-02 -9.64701455e-03 -2.09361743e-02 -1.26805715e-02\n",
      " -4.18429635e-02  9.42103863e-02  9.52017028e-03 -9.59158037e-03\n",
      "  7.63403624e-02 -3.95534895e-02  1.34534657e-01 -1.20037170e-02\n",
      "  3.02774590e-02 -1.14256553e-01  6.59235194e-02 -3.91822234e-02\n",
      "  8.27360302e-02 -1.12790301e-01 -3.03820428e-02  6.80357069e-02\n",
      " -4.43115160e-02 -5.71707375e-02 -2.47084182e-02 -1.29962772e-01\n",
      "  5.49578890e-02 -7.05877766e-02 -1.55707702e-01  5.91692217e-02\n",
      " -5.11182547e-02  5.73860295e-02 -8.55071377e-03  4.50684316e-02\n",
      " -4.44485880e-02 -1.19149834e-01 -6.58917949e-02  8.71871710e-02\n",
      " -4.75563072e-02 -1.24731250e-02  9.56143066e-02  1.44864656e-02\n",
      "  4.91019972e-02 -2.78918389e-02  4.57381867e-02  5.03291823e-02\n",
      "  5.07109761e-02  1.01071030e-01 -2.70120939e-03 -1.83592420e-02\n",
      "  9.29989144e-02  3.98171768e-02  6.95299879e-02 -1.99151300e-02\n",
      " -2.42541805e-01 -1.80021644e-01 -2.41355091e-01  9.86760855e-02\n",
      " -4.26694788e-02  1.70157235e-02 -2.19510756e-02  3.21206301e-02\n",
      " -7.61893615e-02  8.29908922e-02  2.47860197e-02  1.50644526e-01\n",
      "  6.96629956e-02 -9.99497697e-02  9.54934284e-02 -9.84764993e-02\n",
      "  2.15774879e-01 -7.99034834e-02 -6.42167479e-02 -3.95696200e-02\n",
      " -1.73918009e-02  2.25872040e-01 -1.53452614e-02  2.09641922e-02\n",
      "  7.04087764e-02  6.12402509e-04 -8.81666467e-02 -8.70761648e-03\n",
      "  5.84950298e-02  1.55673148e-02 -1.14883617e-01 -7.11837336e-02\n",
      " -6.62312210e-02  2.33291872e-02  1.15138620e-01  4.90476973e-02\n",
      " -3.33371721e-02  1.47251979e-01  4.80112322e-02  1.46796644e-01\n",
      " -9.64862630e-02  7.85965025e-02 -9.53950686e-04  9.21204388e-02\n",
      " -1.41469002e-01  3.62255536e-02  6.11320585e-02  9.06521380e-02\n",
      " -5.30411750e-02  2.91525275e-01  9.32692066e-02 -7.79973939e-02\n",
      " -1.34433284e-01  4.80326191e-02  1.51749521e-01 -1.41121522e-01\n",
      " -1.62135899e-01 -1.36908293e-01  9.03537571e-02 -1.74027100e-01\n",
      "  9.41970572e-02 -2.27921143e-01 -3.25838141e-02  6.82356209e-02\n",
      "  3.02383006e-02 -4.18151021e-02 -1.06648229e-01  9.77820382e-02\n",
      "  3.26321572e-02 -1.09064579e-01  2.01296762e-01  1.91300213e-01\n",
      " -1.43219233e-02  3.55633438e-01 -9.86849144e-02 -9.57025588e-02\n",
      "  1.99755475e-01  2.45579798e-02  9.91487429e-02  1.18182331e-01\n",
      " -3.11627169e-03 -6.93131462e-02  4.69026342e-02 -3.16320173e-02\n",
      "  9.18959361e-03 -2.90671885e-02 -5.93964197e-02 -3.66589278e-02\n",
      "  5.02684973e-02  1.19844705e-01 -3.24602686e-02  5.36915809e-02\n",
      " -1.37470253e-02 -1.49287954e-01 -6.42524064e-02 -2.10716110e-02\n",
      "  8.69956985e-02  1.03839688e-01  4.39196452e-02  7.22898990e-02\n",
      " -1.60273567e-01 -9.98152718e-02 -1.29782721e-01  1.70140453e-02\n",
      " -9.30100381e-02 -5.17619364e-02  1.66776061e-01  6.78254142e-02\n",
      "  1.51747003e-01 -2.75341924e-02  1.81988012e-02  8.89531821e-02\n",
      "  1.34066671e-01 -6.35415614e-02  8.63063037e-02  5.28179966e-02\n",
      " -7.86771402e-02 -3.29226092e-03 -6.42692968e-02 -1.90575063e-01\n",
      "  8.38333666e-02 -1.30441889e-01 -7.65116140e-02  1.96184646e-02\n",
      "  2.31503487e-01  1.02443524e-01  2.47959867e-02 -1.82888508e-01\n",
      "  1.52300760e-01 -3.17806713e-02  2.57404238e-01 -2.33415276e-01\n",
      "  1.92045018e-01 -3.80551778e-02  4.97174636e-02 -7.70002902e-02\n",
      " -8.30702037e-02  7.92703331e-02 -1.92653760e-01  4.12232012e-01\n",
      " -1.38325155e-01  1.02252893e-01  8.19190666e-02  7.18307644e-02\n",
      "  9.61952731e-02  4.60639736e-03  5.20270355e-02 -1.41144097e-01\n",
      " -1.73457891e-01  3.40308584e-02  1.10555381e-01 -9.86847356e-02\n",
      "  1.03496857e-01 -3.40134948e-02 -1.62182391e-01 -6.19310699e-02\n",
      "  2.19253778e-01  5.16350158e-02  6.09978922e-02  1.22997545e-01\n",
      " -3.51517200e-02  1.58531770e-01 -1.01693727e-01 -1.47393076e-02\n",
      " -8.35546777e-02 -1.14358276e-01  1.61328256e-01 -1.95493400e-02\n",
      "  7.28369728e-02 -3.94579917e-02 -2.04227194e-01  1.30492970e-01\n",
      " -2.63571620e-01  1.03822984e-02  7.20876902e-02  4.00388381e-03\n",
      "  3.11367922e-02  1.68579921e-01 -2.26312056e-01 -8.61248095e-03\n",
      " -5.45347594e-02  9.70089659e-02 -2.00030863e-01 -9.60869566e-02\n",
      "  1.03707142e-01  7.41817132e-02  1.20950006e-01  1.69248611e-01\n",
      " -2.01113179e-01 -1.02712609e-01 -3.64068225e-02  1.46776333e-01\n",
      " -5.97356670e-02  1.25636784e-02 -7.96818957e-02  5.03015071e-02\n",
      "  2.61642803e-02 -5.19300066e-03  2.43286253e-03 -7.74755888e-03\n",
      " -1.20847467e-02 -5.45880990e-04 -8.25742260e-03  1.96917467e-02\n",
      " -1.40785590e-01 -1.15763664e-01 -1.70695424e-01 -2.62962421e-03\n",
      " -5.48823699e-02  1.01085261e-01 -9.71278250e-02  1.11157030e-01\n",
      " -3.36121880e-02  2.42749304e-01  1.11740224e-01 -7.24246278e-02\n",
      " -1.90606400e-01  1.24473095e-01  1.52062416e-01 -2.14424700e-01\n",
      "  1.19466847e-02 -1.25098273e-01  3.28639559e-02  2.52354909e-02\n",
      "  3.64803188e-02 -3.01873009e-03  2.15541378e-01  2.86437701e-02\n",
      "  5.80823086e-02 -1.02999300e-01 -2.95194704e-03  2.92617753e-02\n",
      "  2.09759727e-01 -6.29903600e-02 -1.41342029e-01 -1.37630245e-02\n",
      " -5.13175838e-02  9.28385034e-02  8.97132605e-02  6.76978752e-02\n",
      "  2.21232288e-02 -8.57619569e-02  2.18988042e-02 -5.74279800e-02\n",
      "  1.59431636e-01 -2.18953621e-02 -4.64723445e-02 -3.65942121e-02\n",
      "  1.06509596e-01 -7.60487840e-02  2.53674611e-02 -5.50270360e-03\n",
      " -1.59565043e-02  1.55678298e-02  3.10970861e-02 -5.53991459e-02\n",
      " -1.00893900e-01 -6.42698035e-02 -3.30464728e-02  5.69556793e-03\n",
      " -2.54057478e-02  1.44900978e-01 -8.22155401e-02 -1.89291909e-01\n",
      " -9.76485536e-02 -8.74187499e-02  2.18208898e-02  2.10470390e-02\n",
      "  3.77760013e-03  8.44985321e-02 -9.54485461e-02  1.15010805e-01\n",
      " -2.07520679e-01 -1.41710684e-01 -8.92933109e-04  1.69072777e-01\n",
      " -5.65983430e-02  4.77360934e-02 -1.01367734e-01  1.67680800e-01\n",
      "  2.04581209e-02  2.28345886e-01 -1.54844791e-01  9.36456025e-02\n",
      " -2.76855886e-01  1.29332244e-01 -2.13861745e-02 -7.67147765e-02\n",
      " -2.94447155e-03  1.14107719e-02  1.93025488e-02  1.00443460e-01\n",
      "  4.78812568e-02 -1.87657937e-01  1.07213855e-03 -1.88041970e-01\n",
      " -1.86935797e-01 -1.18262293e-02 -2.88761973e-01 -2.01466605e-01\n",
      "  6.68587759e-02 -1.68320999e-01  2.02186301e-01 -1.59235969e-01\n",
      " -9.27796885e-02  6.60468414e-02 -1.41160116e-01  1.28036037e-01\n",
      " -1.39397278e-01 -6.20319135e-02 -7.14158341e-02 -1.37655601e-01\n",
      "  1.26443645e-02  1.72635596e-02  1.41319618e-01  3.82508617e-03\n",
      " -2.15787083e-01 -1.43897220e-01 -1.18297607e-01  2.46600863e-02\n",
      " -1.11975811e-01 -2.18967155e-01 -2.89820302e-02  1.48425505e-01\n",
      "  1.74374327e-01 -1.14953376e-01 -9.59514603e-02 -1.84425563e-02\n",
      "  3.96162152e-01 -2.89707072e-02 -8.59053358e-02  9.72197577e-02\n",
      "  2.97311515e-01  3.88998538e-02 -1.53297596e-02 -5.39052337e-02\n",
      " -1.40163004e-01 -4.83399779e-02  2.74354309e-01  4.73369956e-02\n",
      " -1.04427382e-01  2.70479508e-02 -2.80947071e-02 -1.51543960e-01\n",
      " -1.20248266e-01  7.26754069e-02  2.19958529e-01 -2.48203531e-01\n",
      "  1.55137047e-01 -1.62169710e-01 -2.90108263e-03 -9.83915403e-02\n",
      "  7.46425465e-02 -4.82088365e-02 -6.03797659e-02 -1.44906178e-01\n",
      " -2.01935515e-01  8.73439852e-03  5.56095876e-02  1.60615325e-01\n",
      "  8.66865814e-02  6.03473186e-03 -7.05154017e-02  1.66913029e-02\n",
      "  1.53319299e-01  1.28914416e-01 -5.12815937e-02 -4.56438549e-02\n",
      "  4.12382521e-02 -8.09072331e-02 -9.51811112e-03 -1.40807847e-03\n",
      "  4.92000356e-02 -4.86278087e-02  6.97579533e-02 -3.42570543e-02\n",
      " -1.71424672e-02  6.88067153e-02  1.63867623e-02  9.16411355e-03\n",
      "  2.98981294e-02  9.05247703e-02 -6.27767742e-02  2.45535921e-04\n",
      "  1.00429110e-01 -4.19272445e-02 -1.77766215e-02 -2.90253926e-02\n",
      " -2.52624173e-02  3.04225516e-02 -1.24992616e-01 -1.04886077e-01\n",
      " -1.96277201e-01 -5.95364086e-02 -4.96766418e-02  9.16385576e-02\n",
      " -3.46439630e-02  1.25020921e-01 -2.06788495e-01 -2.06704602e-01\n",
      " -1.03927918e-01 -7.73451030e-02  2.37910282e-02  5.14144264e-02\n",
      "  6.16267174e-02  1.22981071e-02 -8.95846337e-02  2.19444223e-02\n",
      "  1.99762862e-02  9.44310986e-03  8.49599543e-04 -3.84153463e-02\n",
      " -1.41178861e-01 -5.02164708e-03  3.74328084e-02 -1.58305690e-02\n",
      " -9.61323902e-02  8.68288577e-02  3.99705432e-02  1.40697345e-01\n",
      "  5.70342205e-02 -1.15699574e-01  5.57452105e-02  1.27375454e-01\n",
      "  1.04427241e-01  1.03354573e-01  8.11786428e-02 -2.77774762e-02\n",
      "  1.01437457e-01 -2.03351736e-01  7.89487511e-02 -8.30312744e-02\n",
      " -3.83847915e-02 -4.55834419e-02  2.78940573e-02  1.21458597e-01\n",
      "  6.59518875e-03 -5.78706302e-02  1.36520445e-01  6.03801273e-02\n",
      "  1.33296236e-01 -6.01705313e-02  4.66721617e-02  5.94508164e-02\n",
      "  4.25903499e-02 -2.34572113e-01 -3.83734331e-02  1.37596995e-01\n",
      " -1.61147751e-02 -7.08452985e-02  4.72696424e-02 -1.45743951e-01\n",
      " -1.49917100e-02  3.02072763e-02  2.10294351e-02 -4.47751433e-02\n",
      " -1.05916290e-02  5.71398400e-02  8.52873474e-02 -3.56577374e-02\n",
      " -9.45215207e-03 -1.04639463e-01 -2.17977181e-01  1.14148252e-01\n",
      "  2.34275628e-02  1.57105445e-03 -3.28141004e-02 -6.49535134e-02\n",
      "  1.21312821e-03  7.85497949e-03  6.36642128e-02 -1.81412563e-01\n",
      "  9.08747409e-03  1.08576007e-01 -1.00976512e-01  2.11202987e-02\n",
      " -2.69457698e-01  9.98565275e-03 -5.95424138e-02  2.22567335e-01\n",
      "  1.03659794e-01  1.43016398e-01  1.06451802e-01  4.54918891e-02\n",
      " -2.56766006e-02  3.23554054e-02  3.51521634e-02  1.44948319e-01\n",
      " -5.14507741e-02 -1.29428104e-01 -2.82321926e-02 -8.43334273e-02\n",
      " -6.18738793e-02  1.08626693e-01 -1.11864112e-01 -8.66631791e-03\n",
      " -8.26335102e-02  7.24402592e-02  7.06085712e-02  9.86868516e-03\n",
      " -1.30796477e-01  2.69321371e-02 -7.04419017e-02  3.29242684e-02\n",
      "  2.01835617e-01  1.10130897e-02 -1.60033703e-01 -3.10884174e-02\n",
      "  1.11983493e-01 -1.60032019e-01  5.87236732e-02 -6.40852377e-02\n",
      "  4.98596206e-02  9.66151655e-02 -1.77282587e-01 -5.21111228e-02\n",
      "  3.25107686e-02 -8.89155865e-02  6.40772507e-02 -2.79409308e-02\n",
      " -7.52765760e-02 -9.02980790e-02  1.17829688e-01  4.51081768e-02\n",
      "  1.46129608e-01  8.40225518e-02  5.27484752e-02  1.09273523e-01\n",
      "  4.62065823e-02  5.72530366e-03  1.17806524e-01 -3.35619822e-02\n",
      "  1.18259907e-01 -1.81759298e-01  6.59704953e-02 -3.06222513e-02\n",
      " -7.08943680e-02  3.41414846e-02 -5.76046705e-02 -1.45613020e-02\n",
      " -2.55514327e-02 -8.24379623e-02 -1.12279609e-01  4.60627936e-02\n",
      " -1.08930536e-01  1.11848041e-01 -3.30902696e-01  2.15288699e-01\n",
      "  2.07065954e-03 -6.36044517e-02 -4.05055471e-02  3.53099138e-01\n",
      " -6.47729263e-02  3.01441476e-02 -1.71529669e-02 -8.23668391e-02\n",
      "  7.82145932e-02 -1.61703397e-02 -2.04590727e-02  1.31047115e-01\n",
      " -6.15454540e-02  1.82045810e-02 -1.02540687e-01 -6.84434474e-02\n",
      "  1.90975189e-01  2.40584444e-02  1.57391295e-01 -5.30736782e-02\n",
      "  5.97007610e-02  7.55000710e-02  1.33226261e-01  1.31281465e-01\n",
      "  3.28827165e-02 -3.02747427e-03 -2.69725546e-02 -4.92151044e-02]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(sentences=pos_file_content_tokenized+neg_file_content_tokenized, vector_size=768, window=5, min_count=1, workers=16, sg=1)\n",
    "word_embeddings = model.wv\n",
    "\n",
    "\n",
    "word_vector = word_embeddings['movi']\n",
    "print(\"Word vector for 'movi':\", word_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Function to generate document vectors\n",
    "def generate_document_vectors(documents):\n",
    "    document_vectors = []\n",
    "    for doc in documents:\n",
    "        doc_vector = np.mean([word_embeddings[word] for word in doc if word in word_embeddings], axis=0)\n",
    "        document_vectors.append(doc_vector)\n",
    "    return np.array(document_vectors)\n",
    "\n",
    "# Generate document vectors\n",
    "pos_document_vectors = generate_document_vectors(pos_file_content_tokenized)\n",
    "neg_document_vectors = generate_document_vectors(neg_file_content_tokenized)\n",
    "pos_test_document_vectors = generate_document_vectors(pos_test_file_content_tokenized)\n",
    "neg_test_document_vectors = generate_document_vectors(neg_test_file_content_tokenized)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Train SVM classifier\n",
    "x_train = list(pos_document_vectors) + list(neg_document_vectors)\n",
    "X_test = list(pos_test_document_vectors) + list(neg_test_document_vectors)\n",
    "y_train = [1 for _ in range(len(pos_document_vectors))]+[-1 for _ in range(len(neg_document_vectors))]\n",
    "y_test = y_train\n",
    "#print(len(x_train), len(y_train))\n",
    "svm_classifier.fit( x_train , y_train)\n",
    "\n",
    "# Evaluate SVM classifier\n",
    "# accuracy = svm_classifier.score(X_test, y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: LSA\n",
    "In order to perform Latent Semantics Analysis, with a SVM model, we use the document vectors created in the previous section, reduce the dimension to one-third (= 256), and train our Truncated SVM model using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=256)\n",
    "lsa = lsa_model.fit_transform(x_train + X_test)\n",
    "lsa_x_train = lsa[:25000]\n",
    "lsa_x_test = lsa[25000:]\n",
    "\n",
    "svm_lsa_classifier = SVC(kernel='linear')\n",
    "svm_lsa_classifier.fit(lsa_x_train , y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8507438937067388\n",
      "Recall: 0.85544\n",
      "F1-score: 0.8530854840639834\n",
      "Accuracy: 0.85268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Assuming svm_classifier is your trained SVM classifier\n",
    "# Assuming X_test and y_test are your test data and labels respectively\n",
    "\n",
    "y_test = y_train\n",
    "# Predict labels for test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8490506077699214\n",
      "Recall: 0.85496\n",
      "F1-score: 0.8519950572009409\n",
      "Accuracy: 0.85148\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for test data\n",
    "y_pred = svm_lsa_classifier.predict(lsa_x_test)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 13863.26it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 306482.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9522 0 12500 2978\n",
      "Precision 0.43238579602215965\n",
      "Recall 0.76176\n",
      "F1-score 0.551648224320723\n",
      "Accuracy 0.38088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "\n",
    "for test in tqdm.tqdm(pos_test_file_content_tokenized):\n",
    "    prediction = classifier.predict(test)\n",
    "    if (prediction == 1):\n",
    "        TP += 1\n",
    "    else:\n",
    "        FN += 1\n",
    "for test in tqdm.tqdm(neg_test_file_content_tokenized):\n",
    "    if (prediction == 1):\n",
    "        FP += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "\n",
    "print(TP, TN, FP, FN)\n",
    "print(\"Precision\", TP / (TP+FP))\n",
    "print(\"Recall\", TP / (TP+FN))\n",
    "print(\"F1-score\", 2*TP/(2*TP + FP + FN))\n",
    "print(\"Accuracy\", (TP+TN)/(TP+TN+FP+FN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
